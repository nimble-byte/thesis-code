{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e74614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60bf3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import meta_cols, question_dict, type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e362bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tam_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[col for col in df.columns if \"usefulness\" in col or \"ease_of_use\" in col]]\n",
    "\n",
    "def get_self_efficacy_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[col for col in df.columns if \"self_efficacy\" in col]]\n",
    "\n",
    "def get_load_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[col for col in df.columns if \"load\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78dbef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res = pd.read_csv(\"./data/survey_results.csv\", index_col=\"id\", dtype=type_dict, parse_dates=[\"submitdate\", \"startdate\", \"datestamp\"], date_format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "# fix scale for load question 4 (needs to be reversed)\n",
    "raw_res[\"load[SQ004]\"] = 20 - raw_res[\"load[SQ004]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc608db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['group', 'usefulness[SQ001]', 'usefulness[SQ002]', 'usefulness[SQ003]',\n",
       "       'usefulness[SQ004]', 'usefulness[SQ005]', 'usefulness[SQ006]',\n",
       "       'ease_of_use[SQ001]', 'ease_of_use[SQ002]', 'ease_of_use[SQ003]',\n",
       "       'ease_of_use[SQ004]', 'ease_of_use[SQ005]', 'ease_of_use[SQ006]',\n",
       "       'self_efficacy[SQ001]', 'self_efficacy[SQ002]', 'self_efficacy[SQ003]',\n",
       "       'self_efficacy[SQ004]', 'self_efficacy[SQ005]', 'self_efficacy[SQ006]',\n",
       "       'self_efficacy[SQ007]', 'self_efficacy[SQ008]', 'self_efficacy[SQ009]',\n",
       "       'self_efficacy[SQ010]', 'load[SQ001]', 'load[SQ002]', 'load[SQ003]',\n",
       "       'load[SQ004]', 'load[SQ005]', 'load[SQ006]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df = raw_res.drop(columns=[\"lastpage\", \"startlanguage\", \"gender_other\", \"seed\", \"token\", \"refurl\"])\n",
    "\n",
    "meta_df = working_df[meta_cols]\n",
    "data_df = working_df.drop(columns=meta_cols)\n",
    "\n",
    "# sanity check df format\n",
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6362d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce separate output csv\n",
    "expl_df = data_df[raw_res[\"group\"] == \"E\"]\n",
    "base_df = data_df[raw_res[\"group\"] == \"B\"]\n",
    "\n",
    "os.makedirs(\"./out\", exist_ok=True)\n",
    "\n",
    "expl_df.to_csv(\"./out/survey_results_expl.csv\")\n",
    "base_df.to_csv(\"./out/survey_results_base.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f83f7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176576d",
   "metadata": {},
   "source": [
    "The evaluation is performed using simple statistical tests to compare the different groups. The evaluation will be split between the different questionnaires used in the study (TAM, Self-Efficacy, NASA TLX), where TAM is split into its two subscales (Perceived Usefulness and Perceived Ease of Use), CSE is evaluated as a whole, and NASA TLX is evaluated for four of its six subscales (Mental Demand, Performance, Effort, Frustration).\n",
    "\n",
    "The validity of the statistical tests is validated using Mann-Whitney U tests and (Welch's) t-tests for all comparisons. since the small sample size does not allow for a reliable assessment (or assumption) of normality, the non-parametric Mann-Whitney U test is preferred. However, for completeness, t-tests are also reported.\n",
    "\n",
    "Statistical tests are performed using the `scipy.stats` library, and visualizations are created using `seaborn` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbb169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into constructs\n",
    "tam_df = get_tam_df(data_df)\n",
    "self_efficacy_df = get_self_efficacy_df(data_df)\n",
    "load_df = get_load_df(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0915d6",
   "metadata": {},
   "source": [
    "Evaluate NASA TLX results as “raw TLX” as per https://doi.org/10.1177/154193120605000909. Since the given task was not time-constrained and did not involve physical effort, the corresponding subscales are ignored in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0285f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental Demand:\n",
      "  Mann-Whitney U: stat=24.500, p=0.332\n",
      "  t-test: stat=0.507, p=0.311\n",
      "Performance:\n",
      "  Mann-Whitney U: stat=27.000, p=0.213\n",
      "  t-test: stat=1.137, p=0.148\n",
      "Effort:\n",
      "  Mann-Whitney U: stat=21.500, p=0.500\n",
      "  t-test: stat=0.381, p=0.356\n",
      "Frustration:\n",
      "  Mann-Whitney U: stat=37.500, p=0.010\n",
      "  t-test: stat=2.917, p=0.011\n"
     ]
    }
   ],
   "source": [
    "# columns for mental demand, performance, effort, frustration\n",
    "cols = [question_dict[\"load[SQ001]\"], question_dict[\"load[SQ004]\"], question_dict[\"load[SQ005]\"], question_dict[\"load[SQ006]\"]]\n",
    "\n",
    "# rename columns according to question_dict for easier interpretation\n",
    "load_df = load_df.rename(columns=question_dict)\n",
    "\n",
    "# split groups; don't worry about the indices, it looks shady but it's fine)\n",
    "base_group = load_df[raw_res[\"group\"] == \"B\"]\n",
    "expl_group = load_df[raw_res[\"group\"] == \"E\"]\n",
    "\n",
    "for col in cols:\n",
    "    b = base_group[col]\n",
    "    e = expl_group[col]\n",
    "\n",
    "    u_res = stats.mannwhitneyu(b, e, alternative=\"greater\")\n",
    "    u_pval = u_res.pvalue\n",
    "    u_stat = u_res.statistic\n",
    "\n",
    "    t_res = stats.ttest_ind(b, e, equal_var=False, alternative=\"greater\")\n",
    "    t_pval = t_res.pvalue\n",
    "    t_stat = t_res.statistic\n",
    "\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Mann-Whitney U: stat={u_stat:.3f}, p={u_pval:.3f}\")\n",
    "    print(f\"  t-test: stat={t_stat:.3f}, p={t_pval:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
